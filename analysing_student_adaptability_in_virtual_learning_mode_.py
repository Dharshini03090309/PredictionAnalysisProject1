# -*- coding: utf-8 -*-
"""Analysing Student adaptability in Virtual Learning Mode..ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wiKrdY2SHU4OOVqLjts8-lplTEE2679a

PROBLEM STATEMENT:  To examine how well students can adapt online sessions.

LOADING THE DATASET
"""

# reading csv file
import pandas as pd
myData = pd.read_csv('/content/students_adaptability_level_online_education.csv')

print("Dataset Information")
myData.info()

#display first 5 records
myData.head()

print("Column List:")
myData.columns

print("Dimensions:")
myData.shape

print("Finding null values")
myData.isna().sum()

"""NOTE: Not much pre-processing needed in this case as there are no null values hence we begin with next steps establishing sql connection, exploratory data analysis, Model building, training and evaluation.

SQL CONNECTIVITY
"""

#to push dataset into sql table
import sqlite3
import pandas as pd
sqldata = pd.read_csv("/content/students_adaptability_level_online_education.csv")
# Connecting to the database
conn = sqlite3.connect('StudentAdaptabilityOnlineMode.db')
# Insert the data into my table "My_Table"
sqldata.to_sql('My_Table', conn, if_exists='replace', index=False)
# Commit the transaction and close the connection
conn.commit()
conn.close()
print("Success!! Data pushed into the database successfully!!!")

#To display first 15  records pushed into the sql
import sqlite3
import pandas as pd
# Connect to the database
conn = sqlite3.connect('StudentAdaptabilityOnlineMode.db')
# Select the first 15 records from the PredictionTable
query = "SELECT * FROM My_Table LIMIT 15;"
# Reading the sql
ToDisplay = pd.read_sql(query, conn)
conn.close()
# Display
print(ToDisplay)

"""EXPLORATORY DATA ANALYSIS"""

#Gender Distribution Chart
import matplotlib.pyplot as plt
GenderCount = myData['Gender'].value_counts(normalize=True)
colors = ['#66b3ff','#ff66b3']
explode = (0.1, 0.1)
plt.pie(GenderCount.values, labels=GenderCount.index, autopct="%1.1f%%", colors=colors,
        startangle=90, shadow=True, explode=explode, textprops={'fontsize': 14, 'fontweight': 'bold'})
plt.title("Gender Distribution Chart")
plt.legend(loc='center', fontsize=8, title="Gender", title_fontsize=8)
plt.show()

#Institution type Distribution Chart

InstitutionTypeCount = myData['Institution Type'].value_counts(normalize=True)
colors = ['#ffcc99', '#c2c2f0']
explode = (0.1, 0)
plt.pie(InstitutionTypeCount.values, labels=InstitutionTypeCount.index, autopct="%1.1f%%", colors=colors,
        startangle=90, shadow=True, explode=explode, textprops={'fontsize': 14, 'fontweight': 'bold'})
plt.title("Institution Type Distribution Chart")
plt.legend(InstitutionTypeCount.index, loc='center left', fontsize=10, title="Institution Type", title_fontsize=12)
plt.show()

#Education level Distribution Chart

EducationLevelCount = myData['Education Level'].value_counts(normalize=True)
colors = ['#99ccff', '#66ff99', '#ff6666']
explode = (0.1,0.1,0.1)
plt.pie(EducationLevelCount.values, labels=EducationLevelCount.index, autopct="%1.1f%%", colors=colors,
        startangle=90, shadow=True, explode=explode, textprops={'fontsize': 14, 'fontweight': 'bold'})
plt.title("Education Level Distribution Chart")
plt.legend(EducationLevelCount.index, loc='upper left', fontsize=8, title="Education Levels", title_fontsize=12)
plt.show()

#local shedding chart

LocalSheddingCount = myData['Load-shedding'].value_counts(normalize=True)
colors = ['purple', 'violet']
explode = (0.1, 0)
plt.pie(LocalSheddingCount.values, labels=LocalSheddingCount.index, autopct="%1.1f%%", colors=colors,
        startangle=90, shadow=True, explode=explode, textprops={'fontsize': 14, 'fontweight': 'bold'})
plt.title("Local Shedding")
plt.legend(LocalSheddingCount.index, loc='center left', fontsize=10, title="Institution Type", title_fontsize=12)
plt.show()

#financial condition - a visual representation


FinancialConditionCount = myData['Financial Condition'].value_counts(normalize=True)
colors = [ '#ffb3e6', '#c2c2f0', '#99ff99']
explode = (0.1, 0.1, 0.1)
plt.pie(FinancialConditionCount.values, labels=FinancialConditionCount.index, autopct="%1.1f%%", colors=colors,
        startangle=90, shadow=True, explode=explode, textprops={'fontsize': 14, 'fontweight': 'bold'})
plt.title("Financial Condition Chart")
plt.legend(FinancialConditionCount.index, loc='center left', fontsize=10, title="Financial Condition", title_fontsize=12)
plt.show()

#duration - a visual representation

Dur = myData['Class Duration'].unique()
Duration = myData['Class Duration'].value_counts().sort_index()
colors = ['#4caf50', '#2196f3', '#ff5722']
plt.bar(Dur, Duration, color=colors, edgecolor='black', linewidth=1.5)
plt.xlabel('Class Duration (hours)', fontsize=10, fontweight='bold')
plt.ylabel('Number of Students', fontsize=10, fontweight='bold')
plt.title('Bar Chart of Class Duration', fontsize=15, fontweight='bold')
plt.grid(True, axis='y', linestyle='--')
plt.xticks(Dur, fontsize=12, rotation=0)
plt.yticks(fontsize=12)
plt.show()

#internet type - a visualisation

InType = myData['Internet Type'].unique()
InternetType = myData['Internet Type'].value_counts().sort_index()
colors = ['#ff9800', '#9e9e9e']
plt.bar(InType, InternetType, color=colors, edgecolor='black', linewidth=1.5)
plt.xlabel('Internet Type of Students', fontsize=14, fontweight='bold')
plt.ylabel('Number of Students', fontsize=14, fontweight='bold')
plt.title('Distribution of Internet Type of Students in Online Education', fontsize=15, fontweight='bold')
plt.grid(True, axis='y', linestyle='--')
plt.xticks(InType, fontsize=12, rotation=0)
plt.yticks(fontsize=12)
plt.show()

#Adaptavity levels distribution

LevelCount = myData['Adaptivity Level'].value_counts().sort_index()
colors = ['violet','brown','indigo']
plt.bar(LevelCount.index, LevelCount.values, color=colors, edgecolor='black', linewidth=1.5)
plt.xlabel('Adaptability Level of Students', fontsize=14, fontweight='bold')
plt.ylabel('Number of Students', fontsize=14, fontweight='bold')
plt.title('Distribution of Adaptability Levels of Students in Online Education (Target Value)', fontsize=16, fontweight='bold')
plt.grid(True, axis='y', linestyle='--', alpha=0.7)
plt.xticks(LevelCount.index, ['High', 'Low', 'Moderate'], fontsize=12, rotation=0)
plt.yticks(fontsize=12)
plt.show()


highlevelC = LevelCount['High']
print("Total no of students with high adaptivity level:", highlevelC)
ModeratelevelC = LevelCount['Moderate']
print("Total no of students with moderate adaptivity level:", ModeratelevelC)
LowlevelC = LevelCount['Low']
print("Total no of students with Low adaptivity level:", LowlevelC)

#columns represented as a wordcloud
#(no objective: just for fun before moving to the next part!)

import matplotlib.pyplot as plt
from wordcloud import WordCloud
columns = ['Gender', 'Age', 'Education Level', 'Institution Type', 'IT Student',
           'Location', 'Load-shedding', 'Financial Condition', 'Internet Type',
           'Network Type', 'Class Duration', 'Self Lms', 'Device', 'Adaptivity Level']
text = " ".join(columns)
wordcloud = WordCloud(width=700,
    height=1000,
    background_color='black',
    colormap='autumn',
    min_font_size=12
).generate(text)
plt.figure(figsize=(7, 7))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()

# Changing age column in range format to lower limit 'age' format
newAgeColumn = myData["Age"].apply(lambda x: x.split("-")[0])
myData1 = myData.join(newAgeColumn.to_frame(name="Age(lower limit)"))
myData1.drop(['Age'], axis = 1, inplace = True)
myData1.head()
# storing the converted 'age' column into integer type
myData1['Age(lower limit)'] = myData1['Age(lower limit)'].astype(int)
myData1.head()

#Converting categorical to numerical columns and Scaling
#Ordinal encoder converts and stores numerical values in a specified order
from sklearn.preprocessing import OrdinalEncoder
scaling = OrdinalEncoder()
myData1_columns = myData1.columns
transformdata= scaling.fit_transform(myData1)
ScaledData = pd.DataFrame(transformdata, columns=myData1_columns)
ScaledData.head(5)

#Correlation matrix
import seaborn as sns
correlation_matrix = ScaledData.corr()
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True,cmap='RdYlBu', vmin=-1, vmax=1)
plt.title('Correlation Matrix')
plt.show()

#preparing train and test data
train = ScaledData.drop(['Adaptivity Level','Gender',],axis = 1)
test = ScaledData['Adaptivity Level']

#printing dimensions of train and test data separately
print(f"Shape of train set: {x_train.shape}")
print(f"Shape of test set: {x_test.shape}")

# train test split
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test=train_test_split(train,test ,test_size=0.33,random_state=42)

train.head(5)

test.head(5)

from sklearn.metrics import confusion_matrix,accuracy_score

#Random Forest Classifier
from sklearn.ensemble import RandomForestClassifier
model1 = RandomForestClassifier()
# Train the model
model1.fit(x_train, y_train)
# Make predictions
y_pred_model1 = model1.predict(x_test)
print(accuracy_score(y_test,y_pred_model1))

#SVC
from sklearn.svm import SVC
model2 = SVC()
# Train the model
model2.fit(x_train, y_train)
# Make predictions
y_pred_model2 = model2.predict(x_test)
print(accuracy_score(y_test,y_pred_model2))

#Logistic
from sklearn.linear_model import LogisticRegression
model3 = LogisticRegression()
# Train the model
model3.fit(x_train, y_train)
# Make predictions
y_pred_model3 = model3.predict(x_test)
print(accuracy_score(y_test,y_pred_model3))

#Decision Tree
from sklearn.tree import DecisionTreeClassifier
model4 = DecisionTreeClassifier()
# Train the model
model4.fit(x_train, y_train)
# Make predictions
y_pred_model4 = model4.predict(x_test)
print(accuracy_score(y_test,y_pred_model4))
print(confusion_matrix)

from sklearn.metrics import precision_score,recall_score
from sklearn.metrics import f1_score
from sklearn.metrics import confusion_matrix,accuracy_score, classification_report
from sklearn.metrics import ConfusionMatrixDisplay

def getEvaluationMetrics(y_test, y_pred):
    print("--------------------Classification Report--------------------")
    print(classification_report(y_test,y_pred,zero_division=1))
    print(f"Precision: ", round(precision_score(y_test, y_pred, average='macro'),2))
    print(f"Recall: ", round(recall_score(y_test, y_pred, average='macro'),2))
    print(f"F1 Score: ", round(f1_score(y_test, y_pred, average='macro'),2))
    print(f"Accuracy: ", round(accuracy_score(y_test, y_pred),2))

getEvaluationMetrics(y_test, y_pred_model1)

getEvaluationMetrics(y_test, y_pred_model2)

getEvaluationMetrics(y_test, y_pred_model3)

getEvaluationMetrics(y_test, y_pred_model4)

# To represent the importance of features of my selected model - model1
FeatureListImportance=model1.feature_importances_
FeatureListData=pd.DataFrame({'Variable':list(x_train), 'Variable importance':FeatureListImportance})
FeatureListData.sort_values('Variable importance',ascending=False)
plot_importance = pd.Series(model1.feature_importances_, index=train.columns)
plot_importance.nlargest(5).plot(kind='barh')
plt.title("5 most important features")

"""CONCLUSION: Out of 4 models built, Random Forest Classifier Model is the best suited model to predict the levels of adaptablity of students while attending online sessions which gives highest accuracy"""